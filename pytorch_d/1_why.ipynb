{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7959653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "tensor2d_1 = torch.tensor([[1,2,3], [4,5,6]])\n",
    "print(tensor2d_1)\n",
    "tensor2d_2 = torch.tensor([[1,2,3], [4,5,6]])\n",
    "print(tensor2d_2)\n",
    "print(tensor2d_1 @ tensor2d_2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf8b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0f980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58204e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97111d91",
   "metadata": {},
   "source": [
    "## Pytorch is your friend ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70354e8c",
   "metadata": {},
   "source": [
    "1. Tensors -> data structure\n",
    "2. Automatic differentiation engine -> backprop\n",
    "3. Deep learning framework -> training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0e58d",
   "metadata": {},
   "source": [
    "### Answer! ü§î\n",
    "\n",
    "? What is a tensor?\n",
    "\n",
    "? What is autograd?\n",
    "\n",
    "? Efficient data loading? —ç—Ç–æ –∫–∞–∫?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa18c52",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "We use them because they efficiently represent data and operations on it. \n",
    "\n",
    "And we can quickly move data from CPU to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cbffcc",
   "metadata": {},
   "source": [
    "### DL framework\n",
    "\n",
    "1. Pre-trained models\n",
    "2. Loss functions\n",
    "3. Optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8716d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae0b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# tensor0d = torch.tensor(1)\n",
    "# print(tensor0d.dtype)\n",
    "# torchvec =tensor0d.to(torch.bfloat16)\n",
    "# print(torchvec.dtype)\n",
    "# print(torchvec)\n",
    "\n",
    "# tensor0d_float = torch.tensor(1.0)\n",
    "# print(tensor0d_float.dtype)\n",
    "\n",
    "# tensor1d = torch.tensor([1,2,3])\n",
    "# print(tensor1d.dtype)\n",
    "\n",
    "tensor2d = torch.tensor([[1,2,3], [4,5,6]])\n",
    "# print(tensor2d)\n",
    "# print(tensor2d.T)\n",
    "# What is the point of .reshape? it will create a wrong order hmm\n",
    "tensor2d = tensor2d.reshape(1,1,3,2)\n",
    "print(tensor2d.shape)\n",
    "# x = tensor2d.reshape(-1) \n",
    "# print(x)\n",
    "\n",
    "\n",
    "\n",
    "# tensor3d = torch.tensor([[[1,2,3], [4,5,6], [7,8,9]]])\n",
    "# print(tensor3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95183057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32],\n",
       "        [32, 77]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d_1 = torch.tensor([[1,2,3], [4,5,6]])\n",
    "\n",
    "tensor2d_2 = torch.tensor([[1,2,3], [4,5,6]])\n",
    "\n",
    "tensor2d_1 @ tensor2d_2.T # tran\n",
    "# [1, 2, 3]\n",
    "# [4, 5, 6]\n",
    "\n",
    "# [1, 4]\n",
    "# [2, 5]\n",
    "# [3, 6]\n",
    "\n",
    "# r[1, 2, 3] * c[1, 2, 3]\n",
    "# r[1, 2, 3] * c[4, 5, 6]\n",
    "\n",
    "# r[4, 5, 6] * c[1, 2, 3]\n",
    "# r[4, 5, 6] * c[4, 5, 6]\n",
    "\n",
    "# [14, 32]\n",
    "# [32, 77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6819438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409e9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1054, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([-1.1111])\n",
      "tensor(0.0852, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "a = torch.tensor([0.9], requires_grad=True)\n",
    "y = torch.tensor([1.0])\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(loss)\n",
    "grad_L_a = grad(loss, a)[0]\n",
    "print(grad_L_a)\n",
    "\n",
    "# if gradient is > 0, then we need to ‚¨ÜÔ∏è increase weights\n",
    "# if gradient is < 0, then we need to decrease ‚¨áÔ∏è weights\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = w1 * x1 + b\n",
    "a = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "grad_L_w = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    "\n",
    "print(grad_L_w)\n",
    "print(grad_L_b)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3b843",
   "metadata": {},
   "source": [
    "### Multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23523b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(77714791777)\n",
    "\n",
    "model = NeuralNetwork(num_inputs=50, num_outputs=3)\n",
    "model.to(torch.bfloat16)\n",
    "# model.layers[0].weight\n",
    "# model.layers[0].weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e290abb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb72504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1621,  0.1758,  0.1777]], dtype=torch.bfloat16,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1621,  0.1758,  0.1777]], dtype=torch.bfloat16)\n",
      "tensor([[0.2617, 0.3691, 0.3691]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "some_shape = (1, 50)\n",
    "x = torch.randn(some_shape)\n",
    "x = x.to(torch.bfloat16)\n",
    "out = model(x)\n",
    "print(out)\n",
    "\n",
    "# when we don't want to compute and save gradients\n",
    "with torch.no_grad():\n",
    "    out = model(x) \n",
    "print(out)\n",
    "\n",
    "# apply softmax to the output\n",
    "out = torch.softmax(out, dim=1)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0932acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d5202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=50, out_features=30, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553a432",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54110ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "     ])\n",
    "\n",
    "print(X_train.shape)\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6]\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d673de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to the model...\n",
      "‚úÖ Model decided to call a tool.\n",
      "Function to call: schedule_meeting\n",
      "Arguments: {'participants': ['me', 'Vladimir'], 'title': 'Qwen3 Project Plan', 'time': '2023-09-25T14:00:00', 'location': 'Main Conference Room', 'duration_minutes': 45}\n",
      "--- Calling schedule_meeting() ---\n",
      "\n",
      "--- Result ---\n",
      "Meeting Scheduled Successfully!\n",
      "\tTitle: Qwen3 Project Plan\n",
      "\tTime: 2023-09-25T14:00:00\n",
      "\tParticipants: me, Vladimir\n",
      "\tLocation: Main Conference Room\n",
      "\tDuration: 45 minutes\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65451765",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43my_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "y_test.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89affd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2000,  3.1000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_temp = X_train[0]\n",
    "x_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ddc2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.2000,  3.1000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset \n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X \n",
    "        self.labels = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)\n",
    "print(len(train_ds))\n",
    "\n",
    "x0, y0 = train_ds[0]\n",
    "\n",
    "# x0\n",
    "x0\n",
    "# y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57e98828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-1.2000,  3.1000],\n",
      "        [ 2.3000, -1.1000]]) tensor([0, 1])\n",
      "Batch 2: tensor([[-0.5000,  2.6000],\n",
      "        [-0.9000,  2.9000]]) tensor([0, 0])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "426298d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-0.5000,  2.6000],\n",
      "        [-1.2000,  3.1000]]) tensor([0, 0])\n",
      "Batch 2: tensor([[ 2.7000, -1.5000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "# In practice, having a substantially smaller batch as the \n",
    "# last batch in a training epoch can disturb the convergence \n",
    "# during training. To prevent this, it‚Äôs recommended to set drop_last=True,\n",
    "# which will drop the last batch in each epoch, as shown below:\n",
    "# drop_last=True\n",
    "train_loader = DataLoader( \n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74f09c",
   "metadata": {},
   "source": [
    "### Actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Gemma3ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79ab8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ User: What's the weather like in Astana?\n",
      "\n",
      "ü§ñ Model wants to call a tool...\n",
      "Tool calls: [ChatCompletionMessageFunctionToolCall(id='call_db3de021fc434036988cfe', function=Function(arguments='{\"location\": \"Astana\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function', index=0)]\n",
      "\n",
      "Executing function: get_current_weather({'location': 'Astana', 'unit': 'celsius'})\n",
      "Function response: {\"location\": \"Astana\", \"temperature\": \"-15\", \"unit\": \"celsius\", \"forecast\": [\"snowy\", \"windy\", \"cold\"]}\n",
      "\n",
      "üì¢ Sending tool response back to the model...\n",
      "\n",
      "‚úÖ Final Model Response:\n",
      "The current weather in Astana is cold, with a temperature of -15¬∞C. It is snowy and windy, so make sure to dress warmly if you're planning to go outside.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# It's good practice to use environment variables for API keys\n",
    "# For this example, we'll use a placeholder.\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# load_dotenv()\n",
    "# OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"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\", # Replace with your key\n",
    ")\n",
    "\n",
    "# --- Step 1: Define your tool and the function it calls ---\n",
    "\n",
    "# This is the actual Python function that will be executed.\n",
    "def get_current_weather(location, unit=\"celsius\"):\n",
    "    \"\"\"Get the current weather in a given location.\"\"\"\n",
    "    # In a real application, you would call a weather API here.\n",
    "    # For this example, we'll return mock data.\n",
    "    if \"astana\" in location.lower():\n",
    "        weather_info = {\n",
    "            \"location\": location,\n",
    "            \"temperature\": \"-15\",\n",
    "            \"unit\": unit,\n",
    "            \"forecast\": [\"snowy\", \"windy\", \"cold\"],\n",
    "        }\n",
    "    else:\n",
    "        weather_info = {\n",
    "            \"location\": location,\n",
    "            \"temperature\": \"22\",\n",
    "            \"unit\": unit,\n",
    "            \"forecast\": [\"sunny\", \"mild\"],\n",
    "        }\n",
    "    return json.dumps(weather_info)\n",
    "\n",
    "# --- Step 2: Make the first API call with the tools defined ---\n",
    "\n",
    "# The user's prompt that should trigger the tool.\n",
    "user_prompt = \"What's the weather like in Astana?\"\n",
    "print(f\"üë§ User: {user_prompt}\\n\")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "# Describe your tool in the JSON schema format the model expects.\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g., San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# First API call\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"qwen/qwen3-235b-a22b-2507\",\n",
    "  messages=messages,\n",
    "  tools=tools,\n",
    "  tool_choice=\"auto\", # 'auto' lets the model decide, or you can force a tool call.\n",
    ")\n",
    "\n",
    "response_message = completion.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "# --- Step 3: Check if the model wants to call a tool and execute it ---\n",
    "\n",
    "if tool_calls:\n",
    "    print(\"ü§ñ Model wants to call a tool...\")\n",
    "    print(f\"Tool calls: {tool_calls}\\n\")\n",
    "    \n",
    "    # Append the assistant's message with tool calls to the conversation history\n",
    "    messages.append(response_message)\n",
    "    \n",
    "    # In this example, we'll use a mapping to find the correct function.\n",
    "    available_functions = {\n",
    "        \"get_current_weather\": get_current_weather,\n",
    "    }\n",
    "    \n",
    "    # Loop through each tool call the model requested\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        print(f\"Executing function: {function_name}({function_args})\")\n",
    "        \n",
    "        # Call the actual function with the arguments provided by the model\n",
    "        function_response = function_to_call(\n",
    "            location=function_args.get(\"location\"),\n",
    "            unit=function_args.get(\"unit\"),\n",
    "        )\n",
    "        \n",
    "        print(f\"Function response: {function_response}\\n\")\n",
    "\n",
    "        # --- Step 4: Send the function's response back to the model in a second call ---\n",
    "        \n",
    "        # Append the tool's response to the conversation history\n",
    "        messages.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"üì¢ Sending tool response back to the model...\")\n",
    "    \n",
    "    # Second API call\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"qwen/qwen3-235b-a22b-2507\",\n",
    "        messages=messages, # Send the whole conversation history\n",
    "    )\n",
    "    \n",
    "    final_message = second_response.choices[0].message.content\n",
    "    print(f\"\\n‚úÖ Final Model Response:\\n{final_message}\")\n",
    "\n",
    "else:\n",
    "    # If the model didn't call a tool, just print its response\n",
    "    print(f\"ü§ñ Model (no tool call):\\n{response_message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/003 | Batch 000/002 | Train/Val Loss: 1.02\n",
      "Epoch 001/003 | Batch 001/002 | Train/Val Loss: 1.85\n",
      "Epoch 002/003 | Batch 000/002 | Train/Val Loss: 0.07\n",
      "Epoch 002/003 | Batch 001/002 | Train/Val Loss: 0.03\n",
      "Epoch 003/003 | Batch 000/002 | Train/Val Loss: 0.02\n",
      "Epoch 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MLP_neural_network(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 30), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(30, 20),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(20, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(1477741)\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X \n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        out_x = self.features[index]\n",
    "        out_y = self.labels[index]\n",
    "        return out_x, out_y \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "# toy data \n",
    "# train_data\n",
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.6, 2.1],\n",
    "    [2.1, -2.1],\n",
    "    [3.1, -1.4]\n",
    "])\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "# test_data  \n",
    "# shift + option + arrow_up or arrow_down  \n",
    "#  \n",
    "#  \n",
    "#  \n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6]\n",
    "])\n",
    "y_test = torch.tensor([0, 1])\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "\n",
    "model = MLP_neural_network(num_inputs=2, num_outputs=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.5)\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train() # set model to training mode\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels) # loss function\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGS\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:03d}/{NUM_EPOCHS:03d}\"\n",
    "            f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "            f\" | Train/Val Loss: {loss:.2f}\"\n",
    "        )\n",
    "\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dccbdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "temp_1 = torch.tensor([[ 3.2846, -3.1264],\n",
    "        [ 2.9755, -2.8467],\n",
    "        [ 2.0955, -2.1111],\n",
    "        [-2.9865,  1.8815],\n",
    "        [-2.8769,  1.7912]])\n",
    "pred = torch.argmax(temp_1, dim=1)\n",
    "print(pred)\n",
    "# tensor([0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91dca8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.2846, -3.1264],\n",
      "        [ 2.9755, -2.8467],\n",
      "        [ 2.0955, -2.0311],\n",
      "        [-2.9865,  1.8815],\n",
      "        [-2.8769,  1.7912]])\n",
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(predictions)\n",
    "\n",
    "# torch.sum(predictions == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98b9e4",
   "metadata": {},
   "source": [
    "About softmax 51078.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b89085f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0 \n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "    \n",
    "    return (correct / total_examples).item()\n",
    "\n",
    "compute_accuracy(model, test_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8956c",
   "metadata": {},
   "source": [
    "## Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ae1856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,0K\t/home/vladimir_albrekht/LLMs-from-scratch/pytorch_is_easy/model.pth\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "!du -sh /home/vladimir_albrekht/LLMs-from-scratch/pytorch_is_easy/model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1fbdf",
   "metadata": {},
   "source": [
    "#### Restore model      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd38349e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP_neural_network(num_inputs=2, num_outputs=2)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73e948",
   "metadata": {},
   "source": [
    "### GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c31a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32., device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = torch.tensor([1, 2, 3])\n",
    "tensor_1 = tensor_1.to(\"cuda\")\n",
    "tensor_1 = tensor_1.to(torch.bfloat16)\n",
    "tensor_2 = torch.tensor([4, 5, 6])\n",
    "tensor_2 = tensor_2.to(\"cuda:0\")\n",
    "tensor_2 = tensor_2.to(torch.bfloat16)\n",
    "\n",
    "print(tensor_1 @ tensor_2) # 4 + 10 + 18 = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffc662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/003 | Batch 000/003 | Train/Val Loss: 0.78\n",
      "Epoch 001/003 | Batch 001/003 | Train/Val Loss: 0.58\n",
      "Epoch 001/003 | Batch 002/003 | Train/Val Loss: 0.31\n",
      "Epoch 002/003 | Batch 000/003 | Train/Val Loss: 0.03\n",
      "Epoch 002/003 | Batch 001/003 | Train/Val Loss: 0.04\n",
      "Epoch 002/003 | Batch 002/003 | Train/Val Loss: 0.04\n",
      "Epoch 003/003 | Batch 000/003 | Train/Val Loss: 0.01\n",
      "Epoch 003/003 | Batch 001/003 | Train/Val Loss: 0.00\n",
      "Epoch 003/003 | Batch 002/003 | Train/Val Loss: 0.02\n",
      "tensor([[ 4.1386, -2.9504],\n",
      "        [ 2.1535, -1.6710],\n",
      "        [ 3.5200, -2.5943],\n",
      "        [-2.2951,  2.0023],\n",
      "        [-2.8063,  2.4873],\n",
      "        [-3.3176,  2.9723]], device='cuda:0')\n",
      "tensor([0, 0, 0, 1, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class Our_network(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 10),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(1477741)\n",
    "\n",
    "class Our_dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        out_x = self.features[index]\n",
    "        out_y = self.labels[index]\n",
    "        return out_x, out_y \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "# our data\n",
    "X_train = torch.tensor([\n",
    "    [-1.0, 4.2],\n",
    "    [-2.0, 1.0],\n",
    "    [-3.0, 2.0],\n",
    "    [4.0, -3.0],\n",
    "    [5.0, -4.0],\n",
    "    [6.0, -5.0],\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "X_test = torch.tensor([\n",
    "    [-5.0, 4.2],\n",
    "    [3.0, -1.2],\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0, 1])\n",
    "\n",
    "train_dataset = Our_dataset(X_train, y_train)\n",
    "test_dataset = Our_dataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "model = Our_network(num_inputs=2, num_outputs=2)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "# model.to(torch.bfloat16)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "NUM_EPOCHS = 3 \n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        optimizer.zero_grad() # we accumulate the grad so to make sure we will don't have grad_1 + grad_2\n",
    "        loss.backward()\n",
    "        optimizer.step() # update the weights\n",
    "        # Wi = Wi - lr * grad\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch+1:03d}/{NUM_EPOCHS:03d}\"\n",
    "            f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "            f\" | Train/Val Loss: {loss:.2f}\"\n",
    "        )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model = model.eval()\n",
    "    correct = 0.0 \n",
    "    total_examples = 0 \n",
    "    \n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "    \n",
    "    return (correct / total_examples).item()\n",
    "\n",
    "compute_accuracy(model, train_loader)\n",
    "compute_accuracy(model, test_loader)\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(predictions)\n",
    "\n",
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/003| Batch 000/3 | Loss 0.70\n",
      "Epoch 000/003| Batch 001/3 | Loss 11.59\n",
      "Epoch 000/003| Batch 002/3 | Loss 14.90\n",
      "Epoch 001/003| Batch 000/3 | Loss 0.06\n",
      "Epoch 001/003| Batch 001/3 | Loss 1.03\n",
      "Epoch 001/003| Batch 002/3 | Loss 0.01\n",
      "Epoch 002/003| Batch 000/3 | Loss 0.00\n",
      "Epoch 002/003| Batch 001/3 | Loss 0.01\n",
      "Epoch 002/003| Batch 002/3 | Loss 0.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleNetwork_v2(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Multi GPU training\n",
    "\n",
    "import platform\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group # to initialize and destroy the distributed training mods\n",
    "\n",
    "### \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "#@ model \n",
    "class SimpleNetwork_v2(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_outputs)\n",
    "        )\n",
    "        # self.l1 = nn.Linear(num_inputs, 10)\n",
    "        # self.r = nn.ReLU()\n",
    "        # self.l2 = nn.Linear(10,20)\n",
    "        # self.l3 = nn.Linear(20, num_outputs)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.l1(x)\n",
    "        # x = self.r(x)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.r(x)\n",
    "        # logits = self.l3(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "model = SimpleNetwork_v2(num_inputs=2, num_outputs=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.95), lr=0.5, weight_decay=0.1) # it's params 2e-4 = 0.0002 2e-4 -> on the forth index after the comma\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.5) # interesting that lr is affects how fast model will reach the loss = 0\n",
    "# that is because Wi = Wi - lr * grad | and if lr is large and let's say our gradient is -0.5 -> then we update weights like Wi = Wi + 0.5 * 0.5\n",
    "#@ dataset_class specific for the model\n",
    "\n",
    "class SimpleData_v2(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        out_feature = self.features[index]\n",
    "        out_label = self.labels[index]\n",
    "        return out_feature, out_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "# training_data\n",
    "\n",
    "features_train = torch.tensor([\n",
    "    [-0.1, 5.4],\n",
    "    [-0.6, 6.2],\n",
    "    [-1.0, 2.4],\n",
    "    [0.7, -0.9],\n",
    "    [1.2, -0.6],\n",
    "    [1.3, -0.7]\n",
    "])\n",
    "\n",
    "labels_train = torch.tensor([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "train_data = SimpleData_v2(features_train, labels_train)\n",
    "test_data = SimpleData_v2(features_train, labels_train)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "EPOCHS=3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad() # make sure to keep only necessary grads\n",
    "        loss.backward() # calculate\n",
    "        optimizer.step() # update\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d}/{EPOCHS:03d}\"\n",
    "            f\"| Batch {batch_idx:03d}/{len(train_loader)} \"\n",
    "            f\"| Loss {loss:.2f}\"\n",
    "        )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "    \n",
    "    \n",
    "# x, y = train_data\n",
    "# x\n",
    "\n",
    "# features = torch.tensor([\n",
    "#     [0.1, 0.5],\n",
    "#     [0.1, 0.5]\n",
    "# ])\n",
    "# print(features.shape)\n",
    "# logits = model(features)\n",
    "# print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e96ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNetwork_v2(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c469811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0299,  0.1607, -0.1396,  0.0219, -0.0006, -0.1530, -0.1007, -0.0205,\n",
      "        -0.2036, -0.3214])\n"
     ]
    }
   ],
   "source": [
    "temp_tensor_1 = torch.tensor([0.2, 0.1])\n",
    "\n",
    "second_tensor = torch.tensor([\n",
    "    [ 0.3264, -0.3537],\n",
    "    [ 1.8532, -2.0992],\n",
    "    [-0.8408,  0.2854],\n",
    "    [-0.6927,  1.6045],\n",
    "    [-0.4624,  0.9186],\n",
    "    [-0.2607, -1.0090],\n",
    "    [-0.3740, -0.2591],\n",
    "    [ 0.1091, -0.4227],\n",
    "    [-0.7840, -0.4685],\n",
    "    [-1.3080, -0.5980]\n",
    "])\n",
    "\n",
    "print(temp_tensor_1 @ second_tensor.T + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c016bba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3264, -0.3537],\n",
      "        [ 1.8532, -2.0992],\n",
      "        [-0.8408,  0.2854],\n",
      "        [-0.6927,  1.6045],\n",
      "        [-0.4624,  0.9186],\n",
      "        [-0.2607, -1.0090],\n",
      "        [-0.3740, -0.2591],\n",
      "        [ 0.1091, -0.4227],\n",
      "        [-0.7840, -0.4685],\n",
      "        [-1.3080, -0.5980]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(l1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fe4b2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.2059,  1.8140, -0.2335,  0.0586, -1.9925, -1.8767, -0.4438, -2.2289,\n",
       "        -1.7078, -1.1381], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2674be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0299,  0.1607, -0.1396,  0.0219, -0.0006, -0.1530, -0.1007, -0.0205,\n",
      "        -0.2036, -0.3214])\n"
     ]
    }
   ],
   "source": [
    "print(temp_tensor_1 @ second_tensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd9c1c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.1759,  1.9747, -0.3731,  0.0805, -1.9931, -2.0298, -0.5446, -2.2493,\n",
      "        -1.9114, -1.4595], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(l1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d351cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=10, bias=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3fc7aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Layer Linear(in_features=2, out_features=10, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3264, -0.3537],\n",
      "        [ 1.8532, -2.0992],\n",
      "        [-0.8408,  0.2854],\n",
      "        [-0.6927,  1.6045],\n",
      "        [-0.4624,  0.9186],\n",
      "        [-0.2607, -1.0090],\n",
      "        [-0.3740, -0.2591],\n",
      "        [ 0.1091, -0.4227],\n",
      "        [-0.7840, -0.4685],\n",
      "        [-1.3080, -0.5980]], requires_grad=True)\n",
      "\n",
      "--------------------------\n",
      "\n",
      "tensor([-2.1759,  1.9747, -0.3731,  0.0805, -1.9931, -2.0298, -0.5446, -2.2493,\n",
      "        -1.9114, -1.4595], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.9747, 0.0000, 0.0805, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_tensor_1 = torch.tensor([0.2, 0.1])\n",
    "\n",
    "\n",
    "l1 = model.layers[0]\n",
    "print(\"1 Layer\", l1)\n",
    "print(l1.weight)\n",
    "print('\\n--------------------------\\n')\n",
    "l1_out = l1(temp_tensor_1)\n",
    "print(l1_out)\n",
    "relu = model.layers[1]\n",
    "relu(l1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8fa0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNetwork_v2(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_1 = torch.tensor([0.2, -1.2])\n",
    "\n",
    "\n",
    "logits = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4780310e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3DecoderLayer(\n",
       "  (self_attn): Gemma3Attention(\n",
       "    (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "    (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "  )\n",
       "  (mlp): Gemma3MLP(\n",
       "    (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "    (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "    (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "    (act_fn): PytorchGELUTanh()\n",
       "  )\n",
       "  (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "  (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "  (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "  (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_g.language_model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "features()\n",
    "labels\n",
    "logits = model(features)\n",
    "\n",
    "loss = F.cross_entropy(logits, labels)\n",
    "optimizer.zero_grad() # make sure to keep only necessary grads\n",
    "loss.backward() # calculate\n",
    "optimizer.step() # update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26570f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994f890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa498d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir_albrekht/miniconda3/envs/llm_from_scratch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards lol: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_g = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29d307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 6.1021, 5.4498, 0.0000, 5.9007, 0.0000,\n",
       "        0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4227, -1.1018, -0.1979, -4.0171,  6.1021,  5.4498, -4.9131,  5.9007,\n",
      "        -2.7470, -0.0115], grad_fn=<AddBackward0>)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 6.1021, 5.4498, 0.0000, 5.9007, 0.0000,\n",
      "        0.0000], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ef = examples_features = torch.tensor([2.0, -0.5])\n",
    "mL1 = model_weights_0L = model.layers[0].weight\n",
    "wL1 = model.layers[0].bias\n",
    "out_linear = ef @ mL1.T + wL1\n",
    "print(out_linear) # -> Formula inside the model is (x @ w.T + b)\n",
    "ra = torch.relu(out_linear)\n",
    "print(ra)\n",
    "\n",
    "# model.layers[1](out_linear)\n",
    "# mL2 = model.layers[2]\n",
    "# wL2 = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b1eb96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNetwork_v2(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bd4a08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4471, -0.4818],\n",
       "        [ 1.8846, -2.1429],\n",
       "        [-0.9112,  0.3652],\n",
       "        [-0.7811,  1.7149],\n",
       "        [-0.5624,  0.9905],\n",
       "        [-0.3094, -1.0653],\n",
       "        [-0.3553, -0.2462],\n",
       "        [ 0.2469, -0.5475],\n",
       "        [-0.8266, -0.5866],\n",
       "        [-1.3562, -0.6384]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights_before update\n",
    "model.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27061ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1611, -0.0617]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbbecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logtis = [-0.1611, -0.0617]\n",
    "label = 0\n",
    "NLL = 0.1611 + log(exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4a164eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1611, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3502aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8512, 0.9402]], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_1 = torch.exp(logits)\n",
    "print(w_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e209cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens_t = torch.tensor([0.47515909])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2003206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7441])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_log = -torch.log(tens_t)\n",
    "neg_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab22dc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7914, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4781802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8512, 0.9402]], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7441"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1 = torch.exp(logits)\n",
    "print(w_1)\n",
    "sum = torch.sum(w_1)\n",
    "sum\n",
    "\n",
    "log_p = torch.log(sum)\n",
    "NLL1 = (0.1611 + 0.5830)\n",
    "NLL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9ff77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNetwork_v2(num_inputs=2, num_outputs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a13a7cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6059,  0.0071],\n",
       "        [-0.3279, -0.4993],\n",
       "        [-0.6038, -0.6656],\n",
       "        [-0.1631, -0.1670],\n",
       "        [ 0.0542,  0.1671],\n",
       "        [-0.4651, -0.6978],\n",
       "        [ 0.6613, -0.4623],\n",
       "        [-0.7049,  0.0627],\n",
       "        [-0.6422,  0.4389],\n",
       "        [-0.6923,  0.4997]], requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9642b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0027,  0.0137],\n",
       "        [-0.0025, -0.0125],\n",
       "        [ 0.0104,  0.0519],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [-0.0013, -0.0066],\n",
       "        [ 0.0027,  0.0135],\n",
       "        [-0.0018, -0.0091],\n",
       "        [-0.0025, -0.0126],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7441, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.1, 0.5]])\n",
    "labels = torch.tensor([0])\n",
    "logits = model(x)\n",
    "\n",
    "loss = F.cross_entropy(logits, labels)\n",
    "print(loss)\n",
    "optimizer.zero_grad()\n",
    "loss.backward() # that will only create the gradients\n",
    "optimizer.step() # Wi = Wi - lr * grad ||| W1 = -0.6059 - 0.5 * 0.0027\n",
    "# grad defined based on the loss.\n",
    "# Wi = Wi - lr * grad\n",
    "# Bi = Bi - lr * grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b157230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a6ecda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000, -0.0154,  0.0000,  0.0000, -0.0204,\n",
      "         0.0000, -0.0110])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5161,  0.5740],\n",
       "        [ 0.0646, -0.3645],\n",
       "        [-0.3658, -0.0743],\n",
       "        [ 0.4098, -0.0638],\n",
       "        [ 0.4829,  0.0620],\n",
       "        [ 0.0731, -0.5700],\n",
       "        [-0.5840,  0.4129],\n",
       "        [ 0.1945,  0.1713],\n",
       "        [-0.1907, -0.0924],\n",
       "        [ 0.3174, -0.3570]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_of_layer_1 = model.layers[0].weight.grad\n",
    "bias_grads_of_layer_1 = model.layers[0].bias.grad\n",
    "print(bias_grads_of_layer_1)\n",
    "model.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27196539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.]])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor([[1., 2., 3.]])\n",
      "tensor([[14., 14.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0],])\n",
    "x_2 = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [1.0, 2.0, 3.0]])\n",
    "# x_t = torch.tensor([[-0.5295,  0.6235, ])\n",
    "print(x)\n",
    "print(x.T)\n",
    "print(x)\n",
    "print(x @ x_2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.5000]])\n",
      "tensor([[-0.5295, -0.4754,  0.7037, -0.5638, -0.1481, -0.1595, -0.0048, -0.4054,\n",
      "         -0.7043, -0.5226],\n",
      "        [ 0.6235, -0.4820,  0.3126,  0.6699,  0.1003, -0.3440,  0.4915,  0.5695,\n",
      "          0.5585,  0.4490]])\n",
      "tensor([[ 0.2588, -0.2885,  0.2267,  0.2786,  0.0353, -0.1880,  0.2453,  0.2442,\n",
      "          0.2088,  0.1722]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.1, 0.5]])\n",
    "# –∑–Ω–∞—á–∏—Ç —á—Ç–æ –º—ã —É–º–Ω–æ–∞–∂–µ–º 0.1 –∏ 0.5 –Ω–∞ –∫–∞–∂–¥—ã–π –∏–∑ —ç—Ç–∏—Ö –≤–µ–∫—Ç—Ä–æ–≤.\n",
    "x_t = torch.tensor([[-0.5295,  0.6235], # -0.05295 + 0.31175\n",
    "        [-0.4754, -0.4820],\n",
    "        [ 0.7037,  0.3126],\n",
    "        [-0.5638,  0.6699],\n",
    "        [-0.1481,  0.1003],\n",
    "        [-0.1595, -0.3440],\n",
    "        [-0.0048,  0.4915],\n",
    "        [-0.4054,  0.5695],\n",
    "        [-0.7043,  0.5585],\n",
    "        [-0.5226,  0.4490]])\n",
    "print(x)\n",
    "# print(x_t)\n",
    "print(x_t.T)\n",
    "print(x @ x_t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "679825c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.5000]])\n",
      "tensor([[-0.5295,  0.6235],\n",
      "        [-0.4754, -0.4820],\n",
      "        [ 0.7037,  0.3126],\n",
      "        [-0.5638,  0.6699],\n",
      "        [-0.1481,  0.1003],\n",
      "        [-0.1595, -0.3440],\n",
      "        [-0.0048,  0.4915],\n",
      "        [-0.4054,  0.5695],\n",
      "        [-0.7043,  0.5585],\n",
      "        [-0.5226,  0.4490]])\n",
      "tensor([[-0.5295, -0.4754,  0.7037, -0.5638, -0.1481, -0.1595, -0.0048, -0.4054,\n",
      "         -0.7043, -0.5226],\n",
      "        [ 0.6235, -0.4820,  0.3126,  0.6699,  0.1003, -0.3440,  0.4915,  0.5695,\n",
      "          0.5585,  0.4490]])\n",
      "tensor([[ 0.2588, -0.2885,  0.2267,  0.2786,  0.0353, -0.1880,  0.2453,  0.2442,\n",
      "          0.2088,  0.1722]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.1, 0.5]])\n",
    "x_t = torch.tensor([[-0.5295,  0.6235],\n",
    "        [-0.4754, -0.4820],\n",
    "        [ 0.7037,  0.3126],\n",
    "        [-0.5638,  0.6699],\n",
    "        [-0.1481,  0.1003],\n",
    "        [-0.1595, -0.3440],\n",
    "        [-0.0048,  0.4915],\n",
    "        [-0.4054,  0.5695],\n",
    "        [-0.7043,  0.5585],\n",
    "        [-0.5226,  0.4490]])\n",
    "print(x)\n",
    "print(x_t)\n",
    "print(x_t.T)\n",
    "print(x @ x_t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe769679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.5000]])\n",
      "Parameter containing:\n",
      "tensor([[-0.5295,  0.6235],\n",
      "        [-0.4754, -0.4820],\n",
      "        [ 0.7037,  0.3126],\n",
      "        [-0.5638,  0.6699],\n",
      "        [-0.1481,  0.1003],\n",
      "        [-0.1595, -0.3440],\n",
      "        [-0.0048,  0.4915],\n",
      "        [-0.4054,  0.5695],\n",
      "        [-0.7043,  0.5585],\n",
      "        [-0.5226,  0.4490]], requires_grad=True)\n",
      "bais Parameter containing:\n",
      "tensor([ 0.2068, -0.7005,  0.6329,  0.2891,  0.6309,  0.5242, -0.6127,  0.3601,\n",
      "        -0.0094, -0.5742], requires_grad=True)\n",
      "out tensor([[ 0.4656, -0.9891,  0.8596,  0.5677,  0.6662,  0.3362, -0.3674,  0.6044,\n",
      "          0.1994, -0.4019]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.1, 0.5]])  # (1, 2)\n",
    "print(x)\n",
    "weight = model.layers[0].weight  # (10, 2)\n",
    "print(weight)\n",
    "bias = model.layers[0].bias      # (10,)\n",
    "print(\"bais\", bias)\n",
    "\n",
    "output = x @ weight.T + bias  # (1, 10)\n",
    "\n",
    "print(\"out\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078faa6e",
   "metadata": {},
   "source": [
    "## LLM from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8139c91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir_albrekht/miniconda3/envs/llm_from_scratch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (model): Gemma3Model(\n",
       "    (vision_tower): SiglipVisionModel(\n",
       "      (vision_model): SiglipVisionTransformer(\n",
       "        (embeddings): SiglipVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "          (position_embedding): Embedding(4096, 1152)\n",
       "        )\n",
       "        (encoder): SiglipEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-26): 27 x SiglipEncoderLayer(\n",
       "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (self_attn): SiglipAttention(\n",
       "                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): SiglipMLP(\n",
       "                (activation_fn): PytorchGELUTanh()\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    )\n",
       "    (language_model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-33): 34 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_g = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "model_g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8c84b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir_albrekht/miniconda3/envs/llm_from_scratch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 9259]\n",
      "<bos>Hello\n",
      "[2, 175511]\n",
      "<bos>Anime\n",
      "[2, 14447]\n",
      "<bos>World\n",
      "Anime\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "tokenizer.vocab_size\n",
    "\n",
    "data = [\n",
    "    \"Hello\",\n",
    "    \"Anime\",\n",
    "    \"World\"\n",
    "]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    encoded = tokenizer.encode(data[i])\n",
    "    print(encoded)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(decoded)\n",
    "\n",
    "print(data[1])\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "# class MyCustomOptimizer(Optimizer):\n",
    "#     def __init__(self, params, lr=1e-3):\n",
    "#         defaults = dict(lr=lr)\n",
    "#         super().__init__(params, defaults)\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def step(self, closure=None):\n",
    "#         for group in self.param_groups:\n",
    "#             for p in group['params']:\n",
    "#                 if p.grad is not None:\n",
    "#                     p -= group['lr'] * p.grad\n",
    "\n",
    "# optimizer_2 = MyCustomOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12af1bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c507c3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7060, -1.5624, -0.8536,  0.8454],\n",
      "        [ 1.7088,  0.8094, -1.7990,  0.9332],\n",
      "        [-0.2012, -0.4912, -1.3939,  1.2696]])\n",
      "tensor([[ 0.6744, -1.4926, -0.8155,  0.8077],\n",
      "        [ 1.2330,  0.5841, -1.2981,  0.6733],\n",
      "        [-0.2054, -0.5016, -1.4232,  1.2964]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "def _norm(x):\n",
    "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + 1e-6)\n",
    "\n",
    "out = _norm(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ce0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "weight = nn.Parameter(torch.zeros(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11e71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a90339c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5cb236",
   "metadata": {},
   "source": [
    "## Main stuff ü©µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a15c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Gemma3ForCausalLM\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# any model is:\n",
    "# PretrainedConfig\n",
    "# PreTrainedModel\n",
    "# Blocks:\n",
    "#   - Attention\n",
    "#   - MLP \n",
    "#   - RMSNorm\n",
    "#   - Rotary Embedding\n",
    "\n",
    "\n",
    "class KitanConfig(PretrainedConfig):\n",
    "    model_type = \"kitan\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size=262144,\n",
    "            hidden_size=,\n",
    "            num_hidden_layers=,\n",
    "            num_attention_heads=,\n",
    "            num_key_value_heads=,\n",
    "            intermediate_size=,\n",
    "            max_position_embeddings=131_072,\n",
    "            # head_dim=256 none for the 4B model\n",
    "            attention_bias=False\n",
    "            **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "\n",
    "\n",
    "class KitanEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embed_tokens(input_ids)\n",
    "    \n",
    "class KitanRMSNorm(nn.Module):\n",
    "        def __init__(self, dim: int, eps: float = 1e-6):\n",
    "             super().__init__()\n",
    "             self.eps = eps\n",
    "             self.weight = nn.Parameter(torch.zeros(dim))\n",
    "        \n",
    "        def _norm(self, x):\n",
    "             return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "class KitanAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.config = config\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "        #\n",
    "        #\n",
    "\n",
    "        self.q_norm = \n",
    "        self.k_norm = \n",
    "\n",
    "\n",
    "model_ki = KitanModel(\n",
    "     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f544f95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-33): 34 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "          (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Gemma3ForConditionalGeneration, Gemma3ForCausalLM\n",
    "\n",
    "model_g = Gemma3ForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "model_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4dcfda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import SmolLM3ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e24e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.53.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e19c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (4.52.4)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vladimir_albrekht/miniconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "Successfully installed transformers-4.53.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f541df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3,880,263,168\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model_g.parameters())\n",
    "print(f\"Total parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ea043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (language_model): Gemma3TextModel(\n",
    "#     (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
    "#     (layers): ModuleList(\n",
    "#     (0-33): 34 x Gemma3DecoderLayer(\n",
    "#         (self_attn): Gemma3Attention(\n",
    "#         (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
    "#         (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
    "#         (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
    "#         (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
    "#         (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
    "#         (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
    "#         )\n",
    "#         (mlp): Gemma3MLP(\n",
    "#         (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
    "#         (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
    "#         (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
    "#         (act_fn): PytorchGELUTanh()\n",
    "#         )\n",
    "#         (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
    "#         (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
    "#         (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
    "#         (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
    "#     )\n",
    "#     )\n",
    "#     (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
    "#     (rotary_emb): Gemma3RotaryEmbedding()\n",
    "#     (rotary_emb_local): Gemma3RotaryEmbedding()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9673323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1e548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca56a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd4901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695f6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc806a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e48574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbe9506d",
   "metadata": {},
   "source": [
    "### Your mom will be proud of you üêà‚Äç‚¨õ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c4662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 4.2000],\n",
      "        [2.0000, 1.2000]])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74d8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9716ad4d",
   "metadata": {},
   "source": [
    "### Notes üìì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# about crossentropy loss\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "logits = torch.tensor([\n",
    "    [3.0, 1.0, 0.2, 0.1, 1.5, 5.2, 5.5, 1.0],  # –º–µ—Ç–∫–∞ = 0\n",
    "])\n",
    "labels = torch.tensor([2])\n",
    "loss = F.cross_entropy(logits, labels, reduction='mean')\n",
    "print(loss)\n",
    "\n",
    "# something argmax\n",
    "\n",
    "x = torch.tensor([[1.0, 4.2],\n",
    "                 [2.0, 1.2]\n",
    "                 ])\n",
    "print(x)\n",
    "print(torch.argmax(x, dim=0)) # columns\n",
    "\n",
    "print(torch.argmax(x, dim=1)) # rows\n",
    "\n",
    "# count params\n",
    "\n",
    "# for i, p in enumerate(model.parameters()):\n",
    "#     print(model.layers[i])\n",
    "#     print(p.numel())\n",
    "#     if i == len(model.layers) - 1:\n",
    "#         break\n",
    "\n",
    "def count_params(model):\n",
    "    num_params = 0\n",
    "    # print(model)\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            num_params += p.numel()\n",
    "    \n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        print(model.layers[i])\n",
    "        print(p.numel())\n",
    "        if i == len(model.layers) - 1:\n",
    "            break\n",
    "    \n",
    "\n",
    "    print(num_params)\n",
    "    \n",
    "\n",
    "count_params(model)\n",
    "\n",
    "for p in model.layers[0].parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"\\n\\nAfter freezing{'=' * 50}\")\n",
    "count_params(model)\n",
    "\n",
    "# how to use super()\n",
    "\n",
    "class Parent:\n",
    "    def __init__(self):\n",
    "        print(\"Parent __init__ called\")\n",
    "        self.value = 42\n",
    "\n",
    "class Child(Parent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Child __init__ called\")\n",
    "\n",
    "c = Child()\n",
    "print(c.value)\n",
    "\n",
    "class Parent:\n",
    "    def __init__(self):\n",
    "        print(\"Parent __init__ called\")\n",
    "        self.value = 42\n",
    "\n",
    "    def print_value(self):\n",
    "        print(self.value)\n",
    "\n",
    "class Child(Parent):\n",
    "    def __init__(self):\n",
    "        # No super().__init__()\n",
    "        self.value = 100\n",
    "        print(\"Child __init__ called\")\n",
    "\n",
    "c = Child()\n",
    "# print(c.value)\n",
    "c.print_value()\n",
    "\n",
    "\n",
    "# generator expression\n",
    "some_text = \"hello, world!\"\n",
    "print(sum(1 for char in some_text if char == \"l\"))\n",
    "\n",
    "num_params = sum(\n",
    "    print(p.numel()) for p in model.parameters() if p.requires_grad# generator expression\n",
    "    # -\n",
    ")\n",
    "print(\"Total number of trainable model parameters:\", num_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_base (Python 3.12)",
   "language": "python",
   "name": "base_uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
