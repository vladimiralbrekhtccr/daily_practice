{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        eps=1e-6,\n",
    "        bias=False,\n",
    "        qwen3_compatible=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = (\n",
    "            nn.Parameter(torch.zeros(emb_dim)) if bias\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        norm_x = norm_x * self.scale\n",
    "\n",
    "        if self.shift is not None:\n",
    "            norm_x = norm_x + self.shift\n",
    "\n",
    "        return norm_x.to(input_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected argument value expression (3692773971.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mnn.Linear(dtype=)\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected argument value expression\n"
     ]
    }
   ],
   "source": [
    "nn.Linear(dtype=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (fc1): Linear(in_features=40, out_features=1, bias=False)\n",
      "  (fc2): Linear(in_features=40, out_features=1, bias=False)\n",
      "  (fc3): Linear(in_features=1, out_features=40, bias=False)\n",
      ")\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "before fc3:  tensor([0.0825], dtype=torch.bfloat16, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0825, -0.0649, -0.0381,  0.0708,  0.0801, -0.0786,  0.0464, -0.0026,\n",
       "         0.0039,  0.0123, -0.0303, -0.0562, -0.0187,  0.0515, -0.0032, -0.0271,\n",
       "         0.0767, -0.0187, -0.0476, -0.0718, -0.0084, -0.0212,  0.0791, -0.0245,\n",
       "         0.0522, -0.0039,  0.0206,  0.0542, -0.0374, -0.0019,  0.0630, -0.0476,\n",
       "        -0.0737, -0.0090,  0.0283, -0.0167,  0.0535,  0.0620, -0.0737, -0.0212],\n",
       "       dtype=torch.bfloat16, grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "    \"emb_dim\": 40,\n",
    "    \"hidden_dim\": 1,\n",
    "    \"dtype\": torch.bfloat16\n",
    "}\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"],\n",
    "            bias=False\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"],\n",
    "            bias=False\n",
    "        )\n",
    "        self.fc3 = nn.Linear(\n",
    "            cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"],\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        print(x_fc1.shape)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        print(x_fc2.shape)\n",
    "        \n",
    "        # The non-linear activation function here is a SiLU function,           \n",
    "        # which will be discussed later\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        print(\"before fc3: \",x)\n",
    "        print(\"fc2: \",self.fc3.weight)\n",
    "        return self.fc3(x)\n",
    "\n",
    "network_a = FeedForward(cfg)\n",
    "\n",
    "print(network_a)\n",
    "\n",
    "x = torch.randn(40, dtype=cfg[\"dtype\"])\n",
    "network_a(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4062],\n",
       "        [-0.7344],\n",
       "        [ 0.9766],\n",
       "        [ 0.2422],\n",
       "        [ 0.8125],\n",
       "        [-0.3047],\n",
       "        [-0.1172],\n",
       "        [-0.5312],\n",
       "        [-0.4219],\n",
       "        [ 0.2500],\n",
       "        [ 0.0859],\n",
       "        [-0.3125],\n",
       "        [-0.0859],\n",
       "        [ 0.1016],\n",
       "        [ 0.2344],\n",
       "        [-0.8203],\n",
       "        [ 0.4141],\n",
       "        [-0.1875],\n",
       "        [ 0.7031],\n",
       "        [ 0.4922],\n",
       "        [ 0.4297],\n",
       "        [ 0.6172],\n",
       "        [-0.7891],\n",
       "        [ 0.8828],\n",
       "        [ 0.4219],\n",
       "        [ 0.3359],\n",
       "        [ 0.9766],\n",
       "        [ 0.0469],\n",
       "        [-0.7500],\n",
       "        [-0.7188],\n",
       "        [-0.4375],\n",
       "        [ 0.8906],\n",
       "        [ 0.0938],\n",
       "        [ 0.8047],\n",
       "        [-0.7500],\n",
       "        [-0.4844],\n",
       "        [-0.6719],\n",
       "        [ 0.3203],\n",
       "        [-0.6484],\n",
       "        [ 0.0000]], dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_temp = network_a.fc3.weight\n",
    "torch_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8498,  0.5106, -0.2349,  0.9104, -2.0765])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_1 = torch.randn(5)\n",
    "torch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1497])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_2 = torch.randn(1)\n",
    "torch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_3 = torch_1 * torch_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9770,  0.5871, -0.2700,  1.0466, -2.3873])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
